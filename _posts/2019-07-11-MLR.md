---
layout: post
title: Multiple Linear Regressin (MLR) from Scratch
date: 2017-07-11 
description: In this post, I share my python implementations of Multiple Linear Regression (MLR) from scratch.
# Add post description (optional)
img: workflow.jpg # Add image post (optional)
fig-caption: # Add figcaption (optional)
tags: [Machine Learning, Regression, MLR, Python] # add tag
---

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
    });
</script>
<script type="text/javascript" async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



<h1>Table of contents</h1>
<ul>
    <li><a href="#Problem_setup">Problem Setup</a></li>   
    <li><a href="#Data">Data Example</a> </li>  
    <li><a href="#Analytical_solution">MLR with Analytical Solution</a></li>
    <li><a href="#Gradient_descent">MLR with Gradient Descent</a></li>
    <li><a href="#Scikit_learn">MLR with Scikit-Learn</a></li>
</ul> 







# Problem setup  <a name="Problem_setup"></a>

Multiple Linear Regression (MLR), also known as multivariable linear regression, is an extension of Simple Linear Regression (SLR). It is used when we want to predict the value of a dependent variable $ y $ based on the value of $$k \ge 2$$ independent variables $$x_1,...,x_k$$. Suppose we have $n$ observations on the variables,

$$
{y_i} = {\beta _0} + {\beta _1}{x_{i,1}} + ... + {\beta _k}{x_{i,k}} + {e_i},
$$

where $ i = 1, ..., n $, and $ {e_i} $ is a noise variable. In matrix form, we have

$$

{\bf{y}} = {\bf{X\beta }} + {\bf{e}},

$$

`$z = x + y$`

where ${\bf{y}}$ is the vector of observed variables

\begin{equation*}
{\bf{y}} = \left[ \begin{array}{c} 
{{y_1}}\\
{{y_2}}\\
 \vdots \\
{{y_n}} 
\end{array} \right]_{n \times 1},
\end{equation*}

${\bf{X}}$ is a ${n \times \left( {k + 1} \right)}$ matrix of random variables, sometimes called the design matrix, defined as

\begin{equation*}
{\bf{X}} = {\left[ {\begin{array}{l}
1&{{x_{1,1}}}& \cdots &{{x_{1,k}}}\\
1&{{x_{2,1}}}& \cdots &{{x_{2,k}}}\\
 \vdots & \vdots & \ddots & \vdots \\
1&{{x_{n,1}}}& \cdots &{{x_{n,k}}}
\end{array}} \right]_{n \times \left( {k + 1} \right)}},
\end{equation*}

${\bf{\beta }}$ is the parameter vector 

\begin{equation*}
{\bf{\beta }} = {\left[ {\begin{array}{c}
{{\beta _0}}\\
{{\beta _1}}\\
 \vdots \\
{{\beta _k}}
\end{array}} \right]_{\left( {k + 1} \right) \times 1}},
\end{equation*}

and ${\bf{e}}$ is a vector of errors

\begin{equation*}
{\bf{e}} = {\left[ {\begin{array}{c}
{{e_1}}\\
{{e_2}}\\
 \vdots \\
{{e_n}}
\end{array}} \right]_{n \times 1}},
\end{equation*}

and it considered to be generated as ${\bf{e}} \sim {\cal N}\left( {0,{\sigma ^2}{{\bf{I}}_{n \times n}}} \right)$.

Like SLR, we use Mean Squared Error (MSE) metric to estimate parameters ${\bf{\beta }}$. Accordingly, we have the following optimization problem

\begin{equation*}
\underset{{\bf{\beta}}}{\text{minimize}} \quad { J\left( {\bf{\beta }} \right) }
\end{equation*}

where the cost function $J$ is defined as

\begin{equation*}
J = \frac{1}{n}{\left( {{\bf{y}} - {\bf{\hat y}}} \right)^T}\left( {{\bf{y}} - {\bf{\hat y}}} \right) = \frac{1}{n}{{\bf{e}}^T}{\bf{e}},
\end{equation*}

where ${\left( \cdot \right)^{T}}$ denotes transpose operation, ${\bf{y}}$ and ${\bf{\hat y}}$ are actual and predicted values, respectively. In continute, we study analytical and optimization solutions for the above problem.

# Data Example <a name="Data"></a>

In this section, we generate data with the following formula:

\begin{equation*}
y = 30 - 10x_1 + 70x_2 + noise,
\end{equation*}

where $x_1$ and $x_2$ are independe variables with uniform distribution between 0 and 1, while $noise$ has a normal distribution with mean and standard deviation 0 and 2, respectively.


```python
import numpy as np; np.random.seed(123)
# ---------------------------------------------------------
n_samples = 1000           # Number of data samples
n_features = 2             # Number of features
# ---------------------------------------------------------
x = np.random.uniform(0.0, 1.0, (n_samples, n_features))
X = np.hstack((np.ones((n_samples, 1)), x)) 
# ---------------------------------------------------------
mu, sigma = 0, 2           # Mean and standard deviation
noise = np.random.normal(mu, sigma, (n_samples, 1))
# ---------------------------------------------------------
beta = np.array([30, -10, 70])
beta = beta.reshape(len(beta), 1)
# ---------------------------------------------------------
y = X.dot(beta) + noise    # Actual y
```

Let us have a look at the data


